{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Семантическая сегментация. Домашнее задание\n",
        "\n",
        "В этом домашнем задании вам нужно будет реализовать и обучить модели сегментации родинок"
      ],
      "metadata": {
        "id": "grTg9HWJVvq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Датасет"
      ],
      "metadata": {
        "id": "RQIqOOD4V4sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-BDzFMd3Ov",
        "outputId": "ae1e9f47-da21-445e-eaf0-f6bf99f7537a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Для начала мы скачаем датасет: [ADDI project](https://www.fc.up.pt/addi/ph2%20database.html).\n",
        "\n",
        "<table><tr><td><img src=\"data:image/jpeg;base64,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\"></td><td><img src=\"data:image/jpeg;base64,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\"></td></tr></table>\n",
        "\n",
        "2. Разархивируем .rar файл.\n",
        "3. Обратите внимание, что папка  `PH2 Dataset images` должна лежать там же где и ipynb notebook.\n",
        "\n",
        "Это фотографии двух типов **поражений кожи:** меланома и родинки.\n",
        "В данном задании мы не будем заниматься их классификацией, а будем **сегментировать** их."
      ],
      "metadata": {
        "id": "UUuJ-EW3OwpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFPedbog-UFl",
        "outputId": "539727ef-c3f0-42c3-f293-07165db50f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-11 09:26:04--  https://www.dropbox.com/s/k88qukc20ljnbuo/PH2Dataset.rar\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/k88qukc20ljnbuo/PH2Dataset.rar [following]\n",
            "--2024-05-11 09:26:05--  https://www.dropbox.com/s/raw/k88qukc20ljnbuo/PH2Dataset.rar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com/cd/0/inline/CSoeSXcOdoiyjcWCLVTPl4v8ySbk6fl6CmbDF5WLG1Ov3iDPioDOLelScbdiXOVQPngH4ao-DWdW71YOO0uNWXWO9oD4YnevfRjrONcvipm7JzZXymBk227N8Q1eLWrf8Os/file# [following]\n",
            "--2024-05-11 09:26:05--  https://ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com/cd/0/inline/CSoeSXcOdoiyjcWCLVTPl4v8ySbk6fl6CmbDF5WLG1Ov3iDPioDOLelScbdiXOVQPngH4ao-DWdW71YOO0uNWXWO9oD4YnevfRjrONcvipm7JzZXymBk227N8Q1eLWrf8Os/file\n",
            "Resolving ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com (ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com (ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CSoGOP4VVKG8xFRpZ4VJ-nwbnMBDHh6oBmUuMpn0on-pdBZswEQoL7NLRREw-PLbDhD9y7f_9Iabt4WI0PbN7MD1qVhzIfqIYazhd5G2jFgMV40GbFqBpHq36-gemf7geHDkq4L1ocHUgLCDGgwuSWHgzPOZRBQKIlag5CKMLZNV1JhwvkzOf4WRcphzpL-Jht5mjEzG1U4RzTIN50RK4wTJ5dWmdxHB7yJ3Em1Hawgf7OBSAcXSQwNhVqe4w8tkYmcwp7J-PcZDyd5Fnt4ALcfz7YpwsKuUhWUt6y9cMHTReLz6YJG90bSuboRyaTwXgZQ-PhKxBDODXuf2Dm-JQJHDtExcCjfYL5fLSITLTnbeZw/file [following]\n",
            "--2024-05-11 09:26:06--  https://ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com/cd/0/inline2/CSoGOP4VVKG8xFRpZ4VJ-nwbnMBDHh6oBmUuMpn0on-pdBZswEQoL7NLRREw-PLbDhD9y7f_9Iabt4WI0PbN7MD1qVhzIfqIYazhd5G2jFgMV40GbFqBpHq36-gemf7geHDkq4L1ocHUgLCDGgwuSWHgzPOZRBQKIlag5CKMLZNV1JhwvkzOf4WRcphzpL-Jht5mjEzG1U4RzTIN50RK4wTJ5dWmdxHB7yJ3Em1Hawgf7OBSAcXSQwNhVqe4w8tkYmcwp7J-PcZDyd5Fnt4ALcfz7YpwsKuUhWUt6y9cMHTReLz6YJG90bSuboRyaTwXgZQ-PhKxBDODXuf2Dm-JQJHDtExcCjfYL5fLSITLTnbeZw/file\n",
            "Reusing existing connection to ucf59ee994f6a992ea0ac7513346.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 116457882 (111M) [application/rar]\n",
            "Saving to: ‘PH2Dataset.rar’\n",
            "\n",
            "PH2Dataset.rar      100%[===================>] 111.06M   150MB/s    in 0.7s    \n",
            "\n",
            "2024-05-11 09:26:07 (150 MB/s) - ‘PH2Dataset.rar’ saved [116457882/116457882]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw(\"unrar x PH2Dataset.rar\")"
      ],
      "metadata": {
        "id": "zsk-vnrr-VcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls PH2Dataset/PH2\\ Dataset\\ images | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrs2FGv_-WHk",
        "outputId": "16962cd0-7209-46a2-a4c6-3c4755416698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = glob(\"PH2Dataset/PH2 Dataset images/*\")"
      ],
      "metadata": {
        "id": "XygMC95i-WKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1"
      ],
      "metadata": {
        "id": "uWBNTF5rO9zN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Изучите структуру данных и напишите класс датасета для загрузки изображений с их масками сегментации"
      ],
      "metadata": {
        "id": "EsP5x7wqPA7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стуктура датасета у нас следующая:\n",
        "\n",
        "    IMD_002/\n",
        "        IMD002_Dermoscopic_Image/\n",
        "            IMD002.bmp\n",
        "        IMD002_lesion/\n",
        "            IMD002_lesion.bmp\n",
        "        IMD002_roi/\n",
        "            ...\n",
        "    IMD_003/\n",
        "        ...\n",
        "        ...\n",
        "\n",
        " Здесь `X.bmp` — изображение, которое нужно сегментировать, `X_lesion.bmp` — результат сегментации.\n",
        "\n",
        "Для загрузки датасета можно использовать cv2: [`cv2.imread()`](https://www.geeksforgeeks.org/python-opencv-cv2-imread-method/)"
      ],
      "metadata": {
        "id": "Dz8kjiKBPRy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: импортитруйте библиотеки\n",
        "\n",
        "class HumanDataset(Dataset):\n",
        "    def __init__(self, images_paths, transform=None, target_transform=None):\n",
        "        # TODO: задайте аргументы класса\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: верните длину датасета\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO:\n",
        "        # 1. Считайте изображение соответствующее индексу\n",
        "        # 2. Считайте маску сегментации для этого изображения\n",
        "        # 3. Примените трансформ к изображению и маске\n",
        "        # 4. Верните изображение и маску\n",
        "        # Обратите внимание, что значения выходных массивов должны быть типа np.float32 от 0 до 1,\n",
        "        # и размерность маски должна начинаться с кол-ва каналов - [1, 256, 256]\n",
        "        if self.transform:\n",
        "            ...\n",
        "        return ..."
      ],
      "metadata": {
        "id": "_pH9opidebCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2"
      ],
      "metadata": {
        "id": "hdUMFGjvPiS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделите датасет на тренировочную, валидационную и тестовую выборки, создайте объекты класса Dataset.\n",
        "Также выможете дописать свои аугментации в трансформы. Проверьте, что датасет выводит то что нужно с помощью визуализации. Затем создайте даталодеры для всех выборок."
      ],
      "metadata": {
        "id": "16bn2_qvPqin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(256, 256),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(256, 256),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ErDAuak4ebE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# TODO: создайте список путей для всех изображений и разделите его на выборки\n",
        "# TODO: создайте объекты класса HumanDataset для каждой выборки\n",
        "train_dataset = HumanDataset(...)\n",
        "..."
      ],
      "metadata": {
        "id": "eq0i_BHplfux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: визуализируйте элементы датасета\n"
      ],
      "metadata": {
        "id": "W73RL-rrebHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: создайте даталодеры для ваших выборок\n",
        "# Hint: обратите внимание, что валидационную выборку не нужно перемешивать, а батчи должны быть одного размера (в том числе последний)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(...)\n",
        "val_dataloader = DataLoader(...)"
      ],
      "metadata": {
        "id": "C8ec5fdqh_xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Реализация архитектуры U-Net2"
      ],
      "metadata": {
        "id": "exk5OFj3PsYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3"
      ],
      "metadata": {
        "id": "sVjbikOvQgzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Импортируйте необходимые библиотеки"
      ],
      "metadata": {
        "id": "0wzUYkzoQixh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "mrx8drGfhR5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Реализуйте измененную архитектуру U-Net"
      ],
      "metadata": {
        "id": "xif5WnNrQtW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Новая модель путем изменения типа пулинга:\n",
        "\n",
        " **Max-Pooling** for the downsampling and **nearest-neighbor Upsampling** for the upsampling.\n",
        "\n",
        "Down-sampling:\n",
        "\n",
        "        conv = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        pool = nn.MaxPool2d(3, 2, padding=1)\n",
        "\n",
        "Up-Sampling\n",
        "\n",
        "        upsample = nn.Upsample(32)\n",
        "        conv = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "Замените max-pooling на convolutions с stride=2 и upsampling на transpose-convolutions с stride=2."
      ],
      "metadata": {
        "id": "FHn9jJRjDOYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder (downsampling)\n",
        "        self.enc_conv0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # TODO\n",
        "        self.pool0 = ... # 256 -> 128\n",
        "        self.enc_conv1 =  nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # TODO\n",
        "        self.pool1 = ... # 128 -> 64\n",
        "        self.enc_conv2 =  nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # TODO\n",
        "        self.pool2 = ...  # 64 -> 32\n",
        "        self.enc_conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # TODO\n",
        "        self.pool3 =  ...  # 32 -> 16\n",
        "\n",
        "        # bottleneck\n",
        "        self.bottleneck_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # decoder (upsampling)\n",
        "        # TODO\n",
        "        self.upsample0 =  ... # 16 -> 32\n",
        "        self.dec_conv0 =  nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # TODO\n",
        "        self.upsample1 = ... # 32 -> 64\n",
        "        self.dec_conv1 =  nn.Sequential(\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "             nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # TODO\n",
        "        self.upsample2 = ... # 64 -> 128\n",
        "        self.dec_conv2 =  nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # TODO\n",
        "        self.upsample3 = ... # 128 -> 256\n",
        "        self.dec_conv3 =  nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        e0 = self.enc_conv0(x)\n",
        "        e1 = self.enc_conv1(self.pool0(e0))\n",
        "        e2 = self.enc_conv2(self.pool1(e1))\n",
        "        e3 = self.enc_conv3(self.pool2(e2))\n",
        "\n",
        "        # bottleneck\n",
        "        b = self.bottleneck_conv(self.pool3(e3))\n",
        "\n",
        "        # decoder\n",
        "        # concatenate over channels dim=1\n",
        "        d0 = self.dec_conv0(torch.cat([self.upsample0(b), e3], dim=1))\n",
        "        d1 = self.dec_conv1(torch.cat([self.upsample1(d0), e2], dim=1))\n",
        "        d2 = self.dec_conv2(torch.cat([self.upsample2(d1), e1], dim=1))\n",
        "        d3 = self.dec_conv3(torch.cat([self.upsample3(d2), e0], dim=1)) # no activation\n",
        "        return d3"
      ],
      "metadata": {
        "id": "xNBW-NxJP6w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Лосс и метрики"
      ],
      "metadata": {
        "id": "yQiS_uNtSxch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4"
      ],
      "metadata": {
        "id": "WbrLQTVGQ6aL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Focal loss:**](https://arxiv.org/pdf/1708.02002.pdf)\n",
        "\n",
        "Окей, мы уже с вами умеем делать BCE loss:\n",
        "\n",
        "$$\\mathcal L_{BCE}(y, \\hat y) = -\\sum_i \\left[y_i\\log\\sigma(\\hat y_i) + (1-y_i)\\log(1-\\sigma(\\hat y_i))\\right].$$\n",
        "\n",
        "Или в упрощенной форме:\n",
        "\n",
        "$$\\mathcal L_{BCE} = \\hat y - y\\hat y + \\log\\left(1+\\exp(-\\hat y)\\right).$$\n",
        "\n",
        "Проблема с этой потерей заключается в том, что она имеет тенденцию приносить пользу классу **большинства** (фоновому) по отношению к классу **меньшинства** ( переднему). Поэтому обычно применяются весовые коэффициенты к каждому классу:\n",
        "\n",
        "$$\\mathcal L_{wBCE}(y, \\hat y) = -\\sum_i \\alpha_i\\left[y_i\\log\\sigma(\\hat y_i) + (1-y_i)\\log(1-\\sigma(\\hat y_i))\\right].$$\n",
        "\n",
        "Традиционно вес $\\alpha_i$ определяется как обратная частота класса этого пикселя $i$, так что наблюдения минорного класса весят больше по отношению к классу большинства.\n",
        "\n",
        "Еще одним недавним дополнением является взвешенный пиксельный вариант, которая взвешивает каждый пиксель по степени уверенности, которую мы имеем в предсказании этого пикселя.\n",
        "\n",
        "$$\\mathcal L_{focal}(y, \\hat y) = -\\sum_i \\left[\\left(1-\\sigma(\\hat y_i)\\right)^\\gamma y_i\\log\\sigma(\\hat y_i) + \\sigma(\\hat y_i)^\\gamma(1-y_i)\\log(1-\\sigma(\\hat y_i))\\right].$$\n",
        "\n",
        "Зафиксируем значение $\\gamma=2$."
      ],
      "metadata": {
        "id": "ykpl4iDoD0Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(y_real, y_pred, eps = 1e-8, gamma = 2):\n",
        "    # TODO\n",
        "    # Если у вас не получается верно реализовать даную формулу,\n",
        "    # предлагаем поискать и адаптировать готовые реализации и возможные упрощения\n",
        "    return ..."
      ],
      "metadata": {
        "id": "fFCV9RFwS2CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 5"
      ],
      "metadata": {
        "id": "wUaLTdrUQ-Ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dice coefficient:** Учитывая две маски $X$ и $Y$, общая метрика для измерения расстояния между этими двумя масками задается следующим образом:\n",
        "\n",
        "$$D(X,Y)=\\frac{2|X\\cap Y|}{|X|+|Y|}$$\n",
        "\n",
        "Эта функция не является дифференцируемой, но это необходимое свойство для градиентного спуска. В данном случае мы можем приблизить его с помощью:\n",
        "\n",
        "$$\\mathcal L_D(X,Y) = 1-\\frac{1}{256 \\times 256} \\times \\sum_i\\frac{2X_iY_i}{X_i+Y_i}.$$\n",
        "\n",
        "Не забудьте подумать о численной нестабильности, возникающей в математической формуле.\n"
      ],
      "metadata": {
        "id": "onk8UWNSD34u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(y_real, y_pred):\n",
        "    # TODO\n",
        "    return ..."
      ],
      "metadata": {
        "id": "8qqssfEZS2EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрика с семинара"
      ],
      "metadata": {
        "id": "84qr6cK4EKWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    outputs = outputs.squeeze(1).byte()  # BATCH x 1 x H x W => BATCH x H x W\n",
        "    labels = labels.squeeze(1).byte()\n",
        "    SMOOTH = 1e-8\n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Ноль если if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Ноль если оба равны 0\n",
        "\n",
        "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # Избегаем 0/0\n",
        "\n",
        "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # Сравнение с трэшхолдом\n",
        "\n",
        "    return thresholded"
      ],
      "metadata": {
        "id": "msRZs7TeTWpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMQCOoyGS2G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции для обучения"
      ],
      "metadata": {
        "id": "ImDdExigPynM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# визуализация графиков обучения\n",
        "def visualize(train_loss, val_loss, train_score, val_score, title):\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_loss, label = 'train_loss')\n",
        "    plt.plot(val_loss, label = 'val_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_score, label = 'train_score')\n",
        "    plt.plot(val_score, label = 'val_score')\n",
        "    plt.title('Score')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Score')\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "x6JKteofS0tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!export LRU_CACHE_CAPACITY=1"
      ],
      "metadata": {
        "id": "gZ5Of7cBTYdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qnyEMVATYgk",
        "outputId": "d5d93c4c-5cfc-4150-d971-18ee0956ef1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# цикл обучения\n",
        "def train(model, opt, loss_fn, epochs, data_tr, data_val, scheduler, path):\n",
        "    X_val, Y_val = next(iter(data_val))\n",
        "    X_val = X_val.to(device)\n",
        "    Y_val = Y_val.to(device)\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    train_score = []\n",
        "    val_score = []\n",
        "    for epoch in range(epochs):\n",
        "        tic = time()\n",
        "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
        "\n",
        "        avg_loss = 0\n",
        "        avg_train_score = 0\n",
        "        model.train()  # train mode\n",
        "        for X_batch, Y_batch in data_tr:\n",
        "            # перенос на device\n",
        "            X_batch = X_batch.to(device)\n",
        "            Y_batch = Y_batch.to(device)\n",
        "            # обнуляем градиент в оптимизаторе\n",
        "            opt.zero_grad()\n",
        "            # forward\n",
        "            with torch.set_grad_enabled(True):\n",
        "                Y_pred = model(X_batch)\n",
        "\n",
        "                avg_train_score += iou_pytorch(Y_pred>0, Y_batch).mean().item()\n",
        "\n",
        "                loss =  loss_fn(Y_batch, Y_pred)# forward-pass\n",
        "                loss.backward()  # backward-pass\n",
        "                opt.step()  # обновление весов\n",
        "\n",
        "                scheduler.step() # лучше выключить при обучении своей сети без претрейна\n",
        "            # суммируем лосс чтобы вывести график\n",
        "            avg_loss += loss / len(data_tr)\n",
        "\n",
        "        avg_train_score = avg_train_score / len(data_tr)\n",
        "\n",
        "        train_score.append(avg_train_score)\n",
        "        train_loss.append(avg_loss)\n",
        "\n",
        "        toc = time()\n",
        "        print('loss: %f' % avg_loss)\n",
        "\n",
        "        avg_val_loss = 0\n",
        "        avg_val_score = 0\n",
        "        # промежуточные результаты\n",
        "        model.eval()  # testing mode\n",
        "        with torch.no_grad():\n",
        "            for X_val_batch, Y_val_batch in data_val:\n",
        "                # перенос на device\n",
        "                X_val_batch = X_val_batch.to(device)\n",
        "                Y_val_batch = Y_val_batch.to(device)\n",
        "                # forward\n",
        "                Y_val_pred = model(X_val_batch)\n",
        "                vall_loss =  loss_fn(Y_val_batch, Y_val_pred)# forward-pass\n",
        "                avg_val_score += iou_pytorch(Y_val_pred >0, Y_val_batch).mean().item()\n",
        "                # суммируем лосс\n",
        "                avg_val_loss += vall_loss / len(data_val)\n",
        "\n",
        "        val_loss.append(avg_val_loss)\n",
        "        avg_val_score = avg_val_score / len(data_val)\n",
        "        val_score.append(avg_val_score)\n",
        "\n",
        "        Y_hat = model(X_val).detach().cpu() # перенос на cpu\n",
        "\n",
        "        # Визуализация\n",
        "        clear_output(wait=True)\n",
        "        for k in range(6):\n",
        "            plt.subplot(2, 6, k+1)\n",
        "            plt.imshow(np.rollaxis(X_val[k].cpu().numpy(), 0, 3), cmap='gray')\n",
        "            plt.title('Real')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(2, 6, k+7)\n",
        "            plt.imshow(Y_hat[k, 0]>0, cmap='gray')\n",
        "            plt.title('Output')\n",
        "            plt.axis('off')\n",
        "        plt.suptitle('%d / %d - loss: %f' % (epoch+1, epochs, avg_loss))\n",
        "        plt.show()\n",
        "\n",
        "    torch.save(model, path)\n",
        "    return train_loss, val_loss, train_score, val_score"
      ],
      "metadata": {
        "id": "bUxsiw--TfNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, data):\n",
        "    model.eval()  # testing mode\n",
        "    Y_pred = [ X_batch for X_batch, _ in data]\n",
        "    return np.array(Y_pred)\n",
        "\n",
        "def score_model(model, metric, data):\n",
        "    #model.eval()  # testing mode\n",
        "    scores = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_label in data:\n",
        "            Y_pred = model(X_batch.to(device))\n",
        "            scores += metric(Y_pred>0, Y_label.to(device)).mean().item()\n",
        "    return scores/len(data)"
      ],
      "metadata": {
        "id": "a0vFVH2UTfKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_scorerer(model_class, path, max_epochs, loss, data_tr, data_val):\n",
        "    model_name = str(model_class).split()[0][:-1]\n",
        "    loss_name = str(loss).split()[1]\n",
        "    title = f\"Working with {model_name} and {loss_name}\"\n",
        "\n",
        "    model = model_class.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "    train_loss, val_loss, train_score, val_score = \\\n",
        "    train(model, optimizer, loss, max_epochs, data_tr, data_val, scheduler, path)\n",
        "\n",
        "    # print()\n",
        "    # visualize(train_loss.cpu().numpy(), val_loss.cpu().numpy(), train_score, val_score, title)\n",
        "\n",
        "    print(title)\n",
        "    print(f\"validation score: {score_model(model, iou_pytorch, data_val)}\")\n",
        "    return train_loss, val_loss, train_score, val_score, model"
      ],
      "metadata": {
        "id": "zAM9jrZYTp91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение"
      ],
      "metadata": {
        "id": "_o7kvMoVEPLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 6"
      ],
      "metadata": {
        "id": "Ti-FOy_GRPGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучите модель U-Net2 с focal-loss"
      ],
      "metadata": {
        "id": "vMI8rJqNRRzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5Chq66suoo1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_unet_focal, val_loss_unet_focal, train_score_unet_focal, val_score_unet_focal, model_unet_focal = \\\n",
        "model_scorerer(UNet2(), \"unet_focal.pt\", 20, focal_loss, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "QT7x5n3mTqAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_unet_focal = [x.cpu().detach().numpy() for x in train_loss_unet_focal]\n",
        "val_loss_unet_focal = [x.cpu().detach().numpy() for x in val_loss_unet_focal]\n",
        "\n",
        "visualize(train_loss_unet_focal, val_loss_unet_focal,\n",
        "          train_score_unet_focal, val_score_unet_focal, \"UNet + focal\")"
      ],
      "metadata": {
        "id": "D0G6KxlZisaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 7"
      ],
      "metadata": {
        "id": "0cuDWDWlRXwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучите модель U-Net2 с dice-locc"
      ],
      "metadata": {
        "id": "YpHJzj7kRaCt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPlGHAcTRelf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xeer2-q-ReoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JE-KHPSmRfCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы:**"
      ],
      "metadata": {
        "id": "Qtq31qH5RHq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение с помощью mmsegmentation"
      ],
      "metadata": {
        "id": "8sZiJkDtP05p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 8"
      ],
      "metadata": {
        "id": "3We-wPMc0lds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установите необходимые зависимости и импортируйте библиотеки"
      ],
      "metadata": {
        "id": "XV1dHLtGRj5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izcOFnKuVgnB"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 9"
      ],
      "metadata": {
        "id": "0GRHbtgI0oo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишите класс модели, которая будет состоять из бэкбона MobileNetV3 и головы FCNHead"
      ],
      "metadata": {
        "id": "AAi4dKwGRw3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: импортируйте нужные модули из mmseg"
      ],
      "metadata": {
        "id": "KbrBR5ljxOcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.backbone = # TODO\n",
        "        self.head = # TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        # не забудьте увеличить размер выходной маски до размера входного изображения\n",
        "        return ..."
      ],
      "metadata": {
        "id": "gVaT74p2xOfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузите предобученные веса для бэкбона"
      ],
      "metadata": {
        "id": "PpsDLA_WR8w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.openmmlab.com/mmsegmentation/v0.5/mobilenet_v3/lraspp_m-v3s-d8_512x1024_320k_cityscapes/lraspp_m-v3s-d8_512x1024_320k_cityscapes_20201224_223935-61565b34.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vTQfA761yG2",
        "outputId": "f8222a37-989b-4f51-e8b2-e17f864a28a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-11 09:07:50--  https://download.openmmlab.com/mmsegmentation/v0.5/mobilenet_v3/lraspp_m-v3s-d8_512x1024_320k_cityscapes/lraspp_m-v3s-d8_512x1024_320k_cityscapes_20201224_223935-61565b34.pth\n",
            "Resolving download.openmmlab.com (download.openmmlab.com)... 47.246.20.179, 47.246.20.180, 47.246.20.181, ...\n",
            "Connecting to download.openmmlab.com (download.openmmlab.com)|47.246.20.179|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4666964 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘lraspp_m-v3s-d8_512x1024_320k_cityscapes_20201224_223935-61565b34.pth’\n",
            "\n",
            "lraspp_m-v3s-d8_512 100%[===================>]   4.45M  4.44MB/s    in 1.0s    \n",
            "\n",
            "2024-05-11 09:07:52 (4.44 MB/s) - ‘lraspp_m-v3s-d8_512x1024_320k_cityscapes_20201224_223935-61565b34.pth’ saved [4666964/4666964]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_model = TinyModel()"
      ],
      "metadata": {
        "id": "GGOcMU3gxOh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75b055b-79d9-4293-cbcf-7a7b1c4b81c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/mmcv/cnn/bricks/hsigmoid.py:35: UserWarning: In MMCV v1.4.4, we modified the default value of args to align with PyTorch official. Previous Implementation: Hsigmoid(x) = min(max((x + 1) / 2, 0), 1). Current Implementation: Hsigmoid(x) = min(max((x + 3) / 6, 0), 1).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/mmseg/models/decode_heads/decode_head.py:136: UserWarning: threshold is not defined for binary, and defaultsto 0.3\n",
            "  warnings.warn('threshold is not defined for binary, and defaults'\n",
            "/usr/local/lib/python3.10/dist-packages/mmseg/models/builder.py:36: UserWarning: ``build_loss`` would be deprecated soon, please use ``mmseg.registry.MODELS.build()`` \n",
            "  warnings.warn('``build_loss`` would be deprecated soon, please use '\n",
            "/usr/local/lib/python3.10/dist-packages/mmseg/models/losses/cross_entropy_loss.py:250: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = '/content/lraspp_m-v3s-d8_512x1024_320k_cityscapes_20201224_223935-61565b34.pth'\n",
        "w = torch.load(model_name, map_location='cpu');\n",
        "w_backbone = {k.replace('backbone.', ''):v for k,v in w['state_dict'].items() if k.startswith('backbone.')}\n",
        "tiny_model.backbone.load_state_dict(w_backbone)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRoTAEfT1wIz",
        "outputId": "c8b981b0-03cd-4988-d7db-368fb2ef866e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверьте вывод полученной модели"
      ],
      "metadata": {
        "id": "BrCCFuNOSBVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "EuUnejtwDLkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 10\n"
      ],
      "metadata": {
        "id": "CO9IgKUy34E6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучите модель с помощью тех же функций для обучения"
      ],
      "metadata": {
        "id": "skjqOcjBSGWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_scorerer(model, model_name, path, max_epochs, loss, data_tr, data_val):\n",
        "    loss_name = str(loss).split()[1]\n",
        "    title = f\"Working with {model_name} and {loss_name}\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "    train_loss, val_loss, train_score, val_score = \\\n",
        "    train(model, optimizer, loss, max_epochs, data_tr, data_val, scheduler, path)\n",
        "\n",
        "    # print()\n",
        "    # visualize(train_loss.cpu().numpy(), val_loss.cpu().numpy(), train_score, val_score, title)\n",
        "\n",
        "    print(title)\n",
        "    print(f\"validation score: {score_model(model, iou_pytorch, data_val)}\")\n",
        "    return train_loss, val_loss, train_score, val_score, model"
      ],
      "metadata": {
        "id": "_hfTnWH-7IV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss, train_score, val_score, model = \\\n",
        "model_scorerer(tiny_model, \"MobileNet+FCNHead\", \"MobileNet+FCNHead.pt\", 20, focal_loss, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "D9OF1DPX7IYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = [x.cpu().detach().numpy() for x in train_loss]\n",
        "val_loss = [x.cpu().detach().numpy() for x in val_loss]\n",
        "\n",
        "visualize(train_loss, val_loss,\n",
        "          train_score, val_score, \"MobileNetV3 + FCNHead + focal\")"
      ],
      "metadata": {
        "id": "C4Z0i0z37lNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выводы"
      ],
      "metadata": {
        "id": "Gx5jKMEBSNoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 11"
      ],
      "metadata": {
        "id": "kxelV9nxSQ4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сравните ваши эксперименты, сделайте выводы. Ответьте на следующие вопросы:\n",
        "1. Не переобучаются ли ваши нейронные сети?\n",
        "2. Достигаются ли максимально возможные метрики в каждом из экспериментов?\n",
        "3. Как повлияла функция лосса на результаты, скорость и качество обучения?\n",
        "4. Какой эксперимент дает наилучшие резултаты?"
      ],
      "metadata": {
        "id": "_yk3DR9FSSlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGZPd65a8jKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}